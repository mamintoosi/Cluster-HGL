{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import stellargraph as sg\n",
    "from bmdcluster import blockdiagonalBMD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path as osp\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.datasets import DBLP\n",
    "from torch_geometric.nn import SAGEConv, HeteroConv, Linear\n",
    "\n",
    "path = '/mnt/c/temp/working/data/DBLP/'\n",
    "dataset = DBLP(path)\n",
    "data = dataset[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(numpy.ndarray, (4057, 334))"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for key, x in data.x_dict.items() :\n",
    "# #     print(key,len(x), len(x[0]), sum(x[0]), x[0])\n",
    "#     if key == 'author':\n",
    "#         print(key, type(key))\n",
    "#         print(x[0].shape, len(x[0]), sum(x[0]), min(x[0]), max(x[0]))\n",
    "X = data.x_dict['author'].numpy()\n",
    "# X = data.x_dict['paper'].numpy()\n",
    "type(X), X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0 ............. Cost: 217.276\n",
      "Iteration: 1 ............. Cost: 216.409\n",
      "Iteration: 2 ............. Cost: 216.180\n",
      "Iteration: 3 ............. Cost: 216.146\n",
      "Iteration: 4 ............. Cost: 215.905\n",
      "Iteration: 5 ............. Cost: 215.738\n",
      "Iteration: 6 ............. Cost: 215.669\n",
      "Iteration: 7 ............. Cost: 215.620\n",
      "Iteration: 8 ............. Cost: 215.504\n",
      "Iteration: 9 ............. Cost: 215.453\n",
      "Iteration: 10 ............. Cost: 215.421\n",
      "Convergence reached after 12 iterations\n",
      "167\n",
      "68\n",
      "14\n",
      "3\n",
      "987\n",
      "2818\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "K = 10\n",
    "from bmdcluster import generalBMD\n",
    "model = blockdiagonalBMD(n_clusters=K, use_bootstrap=True, b=5)\n",
    "# model = generalBMD(n_clusters=K, B_ident=True, use_bootstrap=True, b=5)\n",
    "model.fit(X, verbose=True)\n",
    "data_labels = model.get_data_labels()\n",
    "# cost, data_clusters, feature_clusters = model.fit_transform(data, verbose=True)\n",
    "for k in range(K):\n",
    "    print(sum(data_labels==k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(['author', 'paper', 'term', 'conference'], [('author', 'to', 'paper'), ('paper', 'to', 'author'), ('paper', 'to', 'term'), ('paper', 'to', 'conference'), ('term', 'to', 'paper'), ('conference', 'to', 'paper')])\n",
      "6\n",
      "[0 1 2 3 4 5]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "only integer scalar arrays can be converted to a scalar index",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-b59bb71ad87b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mr_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetadata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetadata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mr_list\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: only integer scalar arrays can be converted to a scalar index"
     ]
    }
   ],
   "source": [
    "# mask = data['author'].test_mask\n",
    "# Y = X[mask]\n",
    "# Y.shape\n",
    "print(data.metadata())\n",
    "print(len(data.metadata()[1]))\n",
    "r_list = np.arange(len(data.metadata()[1]))\n",
    "print(r_list)\n",
    "print(data.metadata()[1][r_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "249\n",
      "34\n",
      "2592\n",
      "799\n",
      "383\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "K = 5\n",
    "k_means = KMeans(n_clusters=K, random_state=0).fit(X)\n",
    "for k in range(K):\n",
    "    print(sum(k_means.labels_==k))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1616\n",
      "372\n",
      "145\n",
      "174\n",
      "335\n",
      "328\n",
      "21\n",
      "123\n",
      "590\n",
      "353\n"
     ]
    }
   ],
   "source": [
    "# from scipy.cluster.vq import kmeans,vq\n",
    "# centroids, _ = kmeans(X,K)\n",
    "# data_labels,_ = vq(X,centroids)\n",
    "# for k in range(K):\n",
    "#     print(sum(data_labels==k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.8642297"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from popc import popc\n",
    "# data_labels = popc(X)\n",
    "# for k in range(K):\n",
    "#     print(sum(data_labels==k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('author', 'to', 'paper') <class 'tuple'> author\n",
      "19645 tensor(0) tensor(4056)\n",
      "19645 tensor(0) tensor(14327)\n",
      "tensor([[    0,     0,     1,  ...,  4054,  4055,  4056],\n",
      "        [ 2364,  6457,  2365,  ..., 13891, 13891, 13892]])\n"
     ]
    }
   ],
   "source": [
    "# data.metadata()[1]\n",
    "# data.x_dict['author'][0]#, data.edge_index_dict\n",
    "# for key, x in data.x_dict.items() :\n",
    "# #     print(key,len(x), len(x[0]), sum(x[0]), x[0])\n",
    "#     print(key, type(key), key[0])\n",
    "#     print(len(x[0]), min(x[0]), max(x[0]))\n",
    "#     print(len(x[1]), min(x[1]), max(x[1]))\n",
    "#     break\n",
    "#     data.x_dict\n",
    "# data.edge_index_dict\n",
    "\n",
    "for key, x in data.edge_index_dict.items() :\n",
    "    print(key, type(key), key[0])\n",
    "    print(len(x[0]), min(x[0]), max(x[0]))\n",
    "    print(len(x[1]), min(x[1]), max(x[1]))\n",
    "    print(x)\n",
    "    break\n",
    "# print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# شماره نودهای عددی\n",
    "for key, x in data.edge_index_dict.items() :\n",
    "    num_authors = max(x[0]).item()+1\n",
    "    papers_indices = [p.item()+num_authors for p in x[1]]\n",
    "    edges_df = pd.DataFrame(\n",
    "        {\"source\": x[0], \"target\": papers_indices }\n",
    "    )\n",
    "    break\n",
    "edges_df[\"label\"] = \"writes\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a-2'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"a-\"+str(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# شماره نود با پیشوند\n",
    "for key, x in data.edge_index_dict.items() :\n",
    "    authors = [\"a-\"+str(p.item()) for p in x[0]]\n",
    "    papers = [\"p-\"+str(p.item()) for p in x[1]]\n",
    "    edges_df = pd.DataFrame(\n",
    "        {\"source\": authors, \"target\": papers}\n",
    "    )\n",
    "    \n",
    "    authors = [\"a-\"+str(p.item()) for p in np.unique(x[0])]\n",
    "    papers = [\"p-\"+str(p.item()) for p in np.unique(x[1])]\n",
    "    a_df = pd.DataFrame(index = authors)\n",
    "    p_df = pd.DataFrame(index =  papers)\n",
    "    break\n",
    "# edges_df[\"label\"] = \"writes\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a_df\n",
    "# len(authors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StellarGraph: Undirected multigraph\n",
      " Nodes: 18385, Edges: 19645\n",
      "\n",
      " Node types:\n",
      "  papers: [14328]\n",
      "    Features: none\n",
      "    Edge types: papers-default->authors\n",
      "  authors: [4057]\n",
      "    Features: none\n",
      "    Edge types: authors-default->papers\n",
      "\n",
      " Edge types:\n",
      "    authors-default->papers: [19645]\n",
      "        Weights: all 1 (default)\n",
      "        Features: none\n"
     ]
    }
   ],
   "source": [
    "a_p_g = sg.StellarGraph({\"authors\": a_df, \"papers\": p_df}, edges_df)\n",
    "print(a_p_g.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StellarGraph: Undirected multigraph\n",
      " Nodes: 78, Edges: 69\n",
      "\n",
      " Node types:\n",
      "  papers: [68]\n",
      "    Features: none\n",
      "    Edge types: papers-default->authors\n",
      "  authors: [10]\n",
      "    Features: none\n",
      "    Edge types: authors-default->papers\n",
      "\n",
      " Edge types:\n",
      "    authors-default->papers: [69]\n",
      "        Weights: all 1 (default)\n",
      "        Features: none\n"
     ]
    }
   ],
   "source": [
    "# type(x[0]), len(x[0])\n",
    "# x = x[x!=value]\n",
    "# edges_df\n",
    "# num_authors.item(), type(num_authors)\n",
    "# list(edges_df.loc[edges_df[\"source\"]==\"a-0\"].target)\n",
    "selected_authors = [\"a-\"+str(x) for x in range(10)]\n",
    "selected_authors_papers = list(edges_df.loc[edges_df[\"source\"].isin(selected_authors)].target)\n",
    "sub_g = a_p_g.subgraph(selected_authors + list(set(selected_authors_papers)))\n",
    "print(sub_g.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "G_all_nx = nx.from_pandas_edgelist(edges_df)#, edge_attr=\"label\")\n",
    "# nx.set_node_attributes(G_all_nx, \"paper\", \"label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StellarGraph: Undirected multigraph\n",
      " Nodes: 18385, Edges: 19645\n",
      "\n",
      " Node types:\n",
      "  default: [18385]\n",
      "    Features: none\n",
      "    Edge types: default-default->default\n",
      "\n",
      " Edge types:\n",
      "    default-default->default: [19645]\n",
      "        Weights: all 1 (default)\n",
      "        Features: none\n"
     ]
    }
   ],
   "source": [
    "G_all = sg.StellarGraph.from_networkx(G_all_nx)\n",
    "print(G_all.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# شاید بهتر باشه به جای زیرگراف، یک ماسک برای اتصالات انتخابی داشته باشیم\n",
    "# چون در ادامه ماسکهای آموزش و اعتبارسنجی و تست رو داریم. \n",
    "# هر نودی که نمی خواهیم در آموزش مشارکت داشته باشه رو میشه در ماسک آموزش لحاظ کرد\n",
    "# def sub_g(edge_index_dict, d_key, indices):\n",
    "# #     d_key: drop key\n",
    "# #     t_key: tuple key\n",
    "#     for t_key, x in edge_index_dict.items():\n",
    "#         if d_key in t_key:\n",
    "            \n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "with np.load('/mnt/c/temp/working/data/DBLP/raw/adjM.npz') as M:\n",
    "    print(M['indices'])\n",
    "    print(M['indptr'])\n",
    "    print(M['format'])\n",
    "    print(M['shape'])\n",
    "    print(M['data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M.files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HeteroData(\n",
      "  \u001b[1mauthor\u001b[0m={\n",
      "    x=[4057, 334],\n",
      "    y=[4057],\n",
      "    train_mask=[4057],\n",
      "    val_mask=[4057],\n",
      "    test_mask=[4057]\n",
      "  },\n",
      "  \u001b[1mpaper\u001b[0m={ x=[14328, 4231] },\n",
      "  \u001b[1mterm\u001b[0m={ x=[7723, 50] },\n",
      "  \u001b[1mconference\u001b[0m={ num_nodes=20 },\n",
      "  \u001b[1m(author, to, paper)\u001b[0m={ edge_index=[2, 19645] },\n",
      "  \u001b[1m(paper, to, author)\u001b[0m={ edge_index=[2, 19645] },\n",
      "  \u001b[1m(paper, to, term)\u001b[0m={ edge_index=[2, 85810] },\n",
      "  \u001b[1m(paper, to, conference)\u001b[0m={ edge_index=[2, 14328] },\n",
      "  \u001b[1m(term, to, paper)\u001b[0m={ edge_index=[2, 85810] },\n",
      "  \u001b[1m(conference, to, paper)\u001b[0m={ edge_index=[2, 14328] }\n",
      ")\n",
      "Epoch: 001, Loss: 1.3775, Train: 0.5575, Val: 0.4275, Test: 0.4965\n",
      "Epoch: 002, Loss: 1.2896, Train: 0.5875, Val: 0.4650, Test: 0.5327\n",
      "Epoch: 003, Loss: 1.1828, Train: 0.7550, Val: 0.5525, Test: 0.6334\n",
      "Epoch: 004, Loss: 1.0495, Train: 0.8175, Val: 0.6275, Test: 0.6798\n",
      "Epoch: 005, Loss: 0.8928, Train: 0.8850, Val: 0.6675, Test: 0.7123\n",
      "Epoch: 006, Loss: 0.7271, Train: 0.9125, Val: 0.6875, Test: 0.7464\n",
      "Epoch: 007, Loss: 0.5652, Train: 0.9525, Val: 0.7125, Test: 0.7660\n",
      "Epoch: 008, Loss: 0.4162, Train: 0.9750, Val: 0.7500, Test: 0.7912\n",
      "Epoch: 009, Loss: 0.2908, Train: 0.9850, Val: 0.7750, Test: 0.8124\n",
      "CPU times: user 5min 23s, sys: 29 s, total: 5min 52s\n",
      "Wall time: 4min 57s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%run test_01.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HeteroData(\n",
      "  \u001b[1mauthor\u001b[0m={\n",
      "    x=[4057, 334],\n",
      "    y=[4057],\n",
      "    train_mask=[4057],\n",
      "    val_mask=[4057],\n",
      "    test_mask=[4057]\n",
      "  },\n",
      "  \u001b[1mpaper\u001b[0m={ x=[14328, 4231] },\n",
      "  \u001b[1mterm\u001b[0m={ x=[7723, 50] },\n",
      "  \u001b[1mconference\u001b[0m={ num_nodes=20 },\n",
      "  \u001b[1m(author, to, paper)\u001b[0m={ edge_index=[2, 19645] },\n",
      "  \u001b[1m(paper, to, author)\u001b[0m={ edge_index=[2, 19645] },\n",
      "  \u001b[1m(paper, to, term)\u001b[0m={ edge_index=[2, 85810] },\n",
      "  \u001b[1m(paper, to, conference)\u001b[0m={ edge_index=[2, 14328] },\n",
      "  \u001b[1m(term, to, paper)\u001b[0m={ edge_index=[2, 85810] },\n",
      "  \u001b[1m(conference, to, paper)\u001b[0m={ edge_index=[2, 14328] }\n",
      ")\n",
      "Epoch: 001, Loss: 1.3807, Train: 0.5081, Val: 0.4425, Test: 0.5100\n",
      "Epoch: 002, Loss: 1.3149, Train: 0.5867, Val: 0.4925, Test: 0.5375\n",
      "Epoch: 003, Loss: 1.2252, Train: 0.7525, Val: 0.6625, Test: 0.7400\n",
      "Epoch: 004, Loss: 1.1170, Train: 0.8106, Val: 0.7450, Test: 0.7900\n",
      "Epoch: 005, Loss: 0.9861, Train: 0.8465, Val: 0.7800, Test: 0.8150\n",
      "Epoch: 006, Loss: 0.8430, Train: 0.8618, Val: 0.8025, Test: 0.8200\n",
      "Epoch: 007, Loss: 0.7004, Train: 0.8784, Val: 0.8225, Test: 0.8250\n",
      "Epoch: 008, Loss: 0.5699, Train: 0.9014, Val: 0.8275, Test: 0.8425\n",
      "Epoch: 009, Loss: 0.4567, Train: 0.9174, Val: 0.8400, Test: 0.8700\n",
      "Epoch: 010, Loss: 0.3642, Train: 0.9269, Val: 0.8325, Test: 0.8825\n",
      "Epoch: 011, Loss: 0.2949, Train: 0.9352, Val: 0.8400, Test: 0.8825\n",
      "Epoch: 012, Loss: 0.2428, Train: 0.9426, Val: 0.8475, Test: 0.8725\n",
      "Epoch: 013, Loss: 0.2037, Train: 0.9478, Val: 0.8500, Test: 0.8725\n",
      "Epoch: 014, Loss: 0.1746, Train: 0.9549, Val: 0.8475, Test: 0.8700\n",
      "Epoch: 015, Loss: 0.1514, Train: 0.9604, Val: 0.8450, Test: 0.8725\n"
     ]
    }
   ],
   "source": [
    "%run test_02.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch_sparse'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-188490f0a92a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtorch_geometric\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdatasets\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mPlanetoid\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtorch_geometric\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconvert\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mto_networkx\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Programs\\Anaconda3\\envs\\ptch\\lib\\site-packages\\torch_geometric\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mdebug\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mis_debug_enabled\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdebug\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mset_debug\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtorch_geometric\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtorch_geometric\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtorch_geometric\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Programs\\Anaconda3\\envs\\ptch\\lib\\site-packages\\torch_geometric\\data\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mData\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mtemporal\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mTemporalData\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mbatch\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mBatch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mDataset\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0min_memory_dataset\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mInMemoryDataset\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Programs\\Anaconda3\\envs\\ptch\\lib\\site-packages\\torch_geometric\\data\\data.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtorch_geometric\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtorch_sparse\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcoalesce\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSparseTensor\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m from torch_geometric.utils import (contains_isolated_nodes,\n\u001b[0;32m     10\u001b[0m                                    contains_self_loops, is_undirected)\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'torch_sparse'"
     ]
    }
   ],
   "source": [
    "import networkx as nx\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch_geometric.datasets import Planetoid\n",
    "from torch_geometric.utils.convert import to_networkx\n",
    "\n",
    "# dataset1 = Planetoid(root = '/mnt/c/temp/git/tmp/cora',name='Cora')\n",
    "dataset1 = Planetoid(root = 'c:/temp/git/tmp/cora',name='Cora')\n",
    "\n",
    "cora = dataset1 [0]\n",
    "\n",
    "coragraph = to_networkx(cora)\n",
    "\n",
    "node_labels = cora.y[list(coragraph.nodes)].numpy()\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(1,figsize=(14,12)) \n",
    "nx.draw(coragraph, cmap=plt.get_cmap('Set1'),node_color = node_labels,node_size=75,linewidths=6)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gnn_model_summary(model):\n",
    "    \n",
    "    model_params_list = list(model.named_parameters())\n",
    "    print(\"----------------------------------------------------------------\")\n",
    "    line_new = \"{:>50}  {:>15} {:>15}\".format(\"Layer.Parameter\", \"Param Tensor Shape\", \"Param #\")\n",
    "    print(line_new)\n",
    "    print(\"----------------------------------------------------------------\")\n",
    "    for elem in model_params_list:\n",
    "        p_name = elem[0] \n",
    "        p_shape = list(elem[1].size())\n",
    "        p_count = torch.tensor(elem[1].size()).prod().item()\n",
    "        line_new = \"{:>50}  {:>15} {:>15}\".format(p_name, str(p_shape), str(p_count))\n",
    "        print(line_new)\n",
    "    print(\"----------------------------------------------------------------\")\n",
    "    total_params = sum([param.nelement() for param in model.parameters()])\n",
    "    print(\"Total params:\", total_params)\n",
    "    num_trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(\"Trainable params:\", num_trainable_params)\n",
    "    print(\"Non-trainable params:\", total_params - num_trainable_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch_geometric.nn import HeteroConv, GCNConv, SAGEConv, GATConv, Linear\n",
    "\n",
    "# class HeteroGNN(torch.nn.Module):\n",
    "#     def __init__(self, metadata, hidden_channels, out_channels, num_layers):\n",
    "#         super().__init__()\n",
    "\n",
    "#         self.convs = torch.nn.ModuleList()\n",
    "#         # for _ in range(num_layers):\n",
    "#         conv = HeteroConv({\n",
    "#             ('author', 'writes', 'paper'): GCNConv(-1, hidden_channels)\n",
    "#             # ('paper', 'cites', 'paper'): GCNConv(-1, hidden_channels)\n",
    "#             # ('author', 'writes', 'paper'): GCNConv((-1, -1), hidden_channels)\n",
    "#             # ('author', 'affiliated_with', 'institution'): SAGEConv((-1, -1), hidden_channels)\n",
    "#         }, aggr='sum')\n",
    "#         self.convs.append(conv)\n",
    "#             # conv = HeteroConv({\n",
    "#             #     # ('paper', 'cites', 'paper'): GCNConv(-1, hidden_channels)\n",
    "#             #     ('author', 'writes', 'paper'): GCNConv((-1, -1), hidden_channels)\n",
    "#             #     # ('author', 'affiliated_with', 'institution'): SAGEConv((-1, -1), hidden_channels)\n",
    "#             # }, aggr='sum')\n",
    "#             # self.convs.append(conv)\n",
    "\n",
    "#         self.lin = Linear(hidden_channels, out_channels)\n",
    "\n",
    "#     def forward(self, x_dict, edge_index_dict):\n",
    "#         for conv in self.convs:\n",
    "#             x_dict = conv(x_dict, edge_index_dict)\n",
    "#             x_dict = {key: x.relu() for key, x in x_dict.items()}\n",
    "#         return self.lin(x_dict['author'])\n",
    "\n",
    "# # model = HeteroGNN(data.metadata(), hidden_channels=64, out_channels=4,\n",
    "# #                   num_layers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('author', 'to', 'paper'), ('paper', 'to', 'author')]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[edge_type for edge_type in data.metadata()[1][:2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HeteroGNN(torch.nn.Module):\n",
    "    def __init__(self, metadata, hidden_channels, out_channels, num_layers):\n",
    "        super().__init__()\n",
    "\n",
    "        self.convs = torch.nn.ModuleList()\n",
    "        for _ in range(num_layers):\n",
    "            conv = HeteroConv({\n",
    "                edge_type: SAGEConv((-1, -1), hidden_channels)\n",
    "                for edge_type in metadata[1][:2] #انتخاب فقط دو رابطه‌ی اول\n",
    "            })\n",
    "            self.convs.append(conv)\n",
    "\n",
    "        self.lin = Linear(hidden_channels, out_channels)\n",
    "\n",
    "    def forward(self, x_dict, edge_index_dict):\n",
    "        for conv in self.convs:\n",
    "            x_dict = conv(x_dict, edge_index_dict)\n",
    "            x_dict = {key: F.leaky_relu(x) for key, x in x_dict.items()}\n",
    "        return self.lin(x_dict['author'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.loader import NeighborLoader\n",
    "train_input_nodes = ('author', data['author'].train_mask)\n",
    "kwargs = {'batch_size': 64, 'num_workers': 2, 'persistent_workers': True}\n",
    "train_loader = NeighborLoader(data, num_neighbors=[10] * 2, shuffle=True,\n",
    "                              input_nodes=train_input_nodes, **kwargs)\n",
    "model = HeteroGNN(data.metadata(), hidden_channels=64, out_channels=4,\n",
    "                  num_layers=1)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "\n",
    "@torch.no_grad()\n",
    "def init_params():\n",
    "    # Initialize lazy parameters via forwarding a single batch to the model:\n",
    "    batch = next(iter(train_loader))\n",
    "    batch = batch.to(device)\n",
    "    model(batch.x_dict, batch.edge_index_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HeteroGNN(\n",
      "  (convs): ModuleList(\n",
      "    (0): HeteroConv(num_relations=2)\n",
      "  )\n",
      "  (lin): Linear(64, 4, bias=True)\n",
      ")\n",
      "----------------------------------------------------------------\n",
      "                                   Layer.Parameter  Param Tensor Shape         Param #\n",
      "----------------------------------------------------------------\n",
      "      convs.0.convs.author__to__paper.lin_l.weight        [64, 334]           21376\n",
      "        convs.0.convs.author__to__paper.lin_l.bias             [64]              64\n",
      "      convs.0.convs.author__to__paper.lin_r.weight       [64, 4231]          270784\n",
      "      convs.0.convs.paper__to__author.lin_l.weight       [64, 4231]          270784\n",
      "        convs.0.convs.paper__to__author.lin_l.bias             [64]              64\n",
      "      convs.0.convs.paper__to__author.lin_r.weight        [64, 334]           21376\n",
      "                                        lin.weight          [4, 64]             256\n",
      "                                          lin.bias              [4]               4\n",
      "----------------------------------------------------------------\n",
      "Total params: 584708\n",
      "Trainable params: 584708\n",
      "Non-trainable params: 0\n"
     ]
    }
   ],
   "source": [
    "# print(model)\n",
    "# init_params()  # Initialize parameters.\n",
    "with torch.no_grad():  # Initialize lazy modules.\n",
    "    out = model(data.x_dict, data.edge_index_dict)\n",
    "\n",
    "print(model)\n",
    "gnn_model_summary(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 1.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.x_dict['author']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "acb5fe38fbd28c402c703fdbe5b19100300a03e07d3a6753113d2269daa5b738"
  },
  "kernelspec": {
   "display_name": "Python 3.7.6 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
