{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import stellargraph as sg\n",
    "from bmdcluster import blockdiagonalBMD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "No such operator torch_sparse::hetero_neighbor_sample",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-125-2429dad39a28>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;31m# Use a batch size of 128 for sampling training nodes of type \"paper\":\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0minput_nodes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'paper'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'paper'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m )\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch_geometric/loader/neighbor_loader.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, num_neighbors, input_nodes, replace, directed, transform, **kwargs)\u001b[0m\n\u001b[1;32m    157\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# `HeteroData`:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnode_types\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0medge_types\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetadata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 159\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtorch_sparse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhetero_neighbor_sample\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    160\u001b[0m             \u001b[0;31m# Convert the graph data into a suitable format for sampling.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m             \u001b[0;31m# NOTE: Since C++ cannot take dictionaries with tuples as key as\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/_ops.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, op_name)\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0;31m# for overloads and raise an exception if there are more than one.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0mqualified_op_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'{}::{}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m         \u001b[0mop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_get_operation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mqualified_op_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m         \u001b[0;31m# let the script frontend know that op is identical to the builtin op\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;31m# with qualified_op_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: No such operator torch_sparse::hetero_neighbor_sample"
     ]
    }
   ],
   "source": [
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.datasets import OGB_MAG\n",
    "from torch_geometric.loader import NeighborLoader\n",
    "\n",
    "transform = T.ToUndirected()  # Add reverse edge types.\n",
    "data = OGB_MAG(root='/mnt/c/temp/working/data', preprocess='metapath2vec', transform=transform)[0]\n",
    "\n",
    "train_loader = NeighborLoader(\n",
    "    data,\n",
    "    # Sample 15 neighbors for each node and each edge type for 2 iterations:\n",
    "    num_neighbors=[15] * 2,\n",
    "    # Use a batch size of 128 for sampling training nodes of type \"paper\":\n",
    "    batch_size=128,\n",
    "    input_nodes=('paper', data['paper'].train_mask),\n",
    ")\n",
    "\n",
    "batch = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(numpy.ndarray, (4057, 334))"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for key, x in data.x_dict.items() :\n",
    "# #     print(key,len(x), len(x[0]), sum(x[0]), x[0])\n",
    "#     if key == 'author':\n",
    "#         print(key, type(key))\n",
    "#         print(x[0].shape, len(x[0]), sum(x[0]), min(x[0]), max(x[0]))\n",
    "X = data.x_dict['author'].numpy()\n",
    "# X = data.x_dict['paper'].numpy()\n",
    "type(X), X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0 ............. Cost: 217.276\n",
      "Iteration: 1 ............. Cost: 216.409\n",
      "Iteration: 2 ............. Cost: 216.180\n",
      "Iteration: 3 ............. Cost: 216.146\n",
      "Iteration: 4 ............. Cost: 215.905\n",
      "Iteration: 5 ............. Cost: 215.738\n",
      "Iteration: 6 ............. Cost: 215.669\n",
      "Iteration: 7 ............. Cost: 215.620\n",
      "Iteration: 8 ............. Cost: 215.504\n",
      "Iteration: 9 ............. Cost: 215.453\n",
      "Iteration: 10 ............. Cost: 215.421\n",
      "Convergence reached after 12 iterations\n",
      "167\n",
      "68\n",
      "14\n",
      "3\n",
      "987\n",
      "2818\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "K = 10\n",
    "from bmdcluster import generalBMD\n",
    "model = blockdiagonalBMD(n_clusters=K, use_bootstrap=True, b=5)\n",
    "# model = generalBMD(n_clusters=K, B_ident=True, use_bootstrap=True, b=5)\n",
    "model.fit(X, verbose=True)\n",
    "data_labels = model.get_data_labels()\n",
    "# cost, data_clusters, feature_clusters = model.fit_transform(data, verbose=True)\n",
    "for k in range(K):\n",
    "    print(sum(data_labels==k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['author', 'paper', 'term', 'conference'],\n",
       " [('author', 'to', 'paper'),\n",
       "  ('paper', 'to', 'author'),\n",
       "  ('paper', 'to', 'term'),\n",
       "  ('paper', 'to', 'conference'),\n",
       "  ('term', 'to', 'paper'),\n",
       "  ('conference', 'to', 'paper')])"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# mask = data['author'].test_mask\n",
    "# Y = X[mask]\n",
    "# Y.shape\n",
    "data.metadata()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "249\n",
      "34\n",
      "2592\n",
      "799\n",
      "383\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "K = 5\n",
    "k_means = KMeans(n_clusters=K, random_state=0).fit(X)\n",
    "for k in range(K):\n",
    "    print(sum(k_means.labels_==k))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1616\n",
      "372\n",
      "145\n",
      "174\n",
      "335\n",
      "328\n",
      "21\n",
      "123\n",
      "590\n",
      "353\n"
     ]
    }
   ],
   "source": [
    "# from scipy.cluster.vq import kmeans,vq\n",
    "# centroids, _ = kmeans(X,K)\n",
    "# data_labels,_ = vq(X,centroids)\n",
    "# for k in range(K):\n",
    "#     print(sum(data_labels==k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.8642297"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from popc import popc\n",
    "# data_labels = popc(X)\n",
    "# for k in range(K):\n",
    "#     print(sum(data_labels==k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('author', 'to', 'paper') <class 'tuple'> author\n",
      "19645 tensor(0) tensor(4056)\n",
      "19645 tensor(0) tensor(14327)\n",
      "tensor([[    0,     0,     1,  ...,  4054,  4055,  4056],\n",
      "        [ 2364,  6457,  2365,  ..., 13891, 13891, 13892]])\n"
     ]
    }
   ],
   "source": [
    "# data.metadata()[1]\n",
    "# data.x_dict['author'][0]#, data.edge_index_dict\n",
    "# for key, x in data.x_dict.items() :\n",
    "# #     print(key,len(x), len(x[0]), sum(x[0]), x[0])\n",
    "#     print(key, type(key), key[0])\n",
    "#     print(len(x[0]), min(x[0]), max(x[0]))\n",
    "#     print(len(x[1]), min(x[1]), max(x[1]))\n",
    "#     break\n",
    "#     data.x_dict\n",
    "# data.edge_index_dict\n",
    "\n",
    "for key, x in data.edge_index_dict.items() :\n",
    "    print(key, type(key), key[0])\n",
    "    print(len(x[0]), min(x[0]), max(x[0]))\n",
    "    print(len(x[1]), min(x[1]), max(x[1]))\n",
    "    print(x)\n",
    "    break\n",
    "# print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# شماره نودهای عددی\n",
    "for key, x in data.edge_index_dict.items() :\n",
    "    num_authors = max(x[0]).item()+1\n",
    "    papers_indices = [p.item()+num_authors for p in x[1]]\n",
    "    edges_df = pd.DataFrame(\n",
    "        {\"source\": x[0], \"target\": papers_indices }\n",
    "    )\n",
    "    break\n",
    "edges_df[\"label\"] = \"writes\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a-2'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"a-\"+str(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# شماره نود با پیشوند\n",
    "for key, x in data.edge_index_dict.items() :\n",
    "    authors = [\"a-\"+str(p.item()) for p in x[0]]\n",
    "    papers = [\"p-\"+str(p.item()) for p in x[1]]\n",
    "    edges_df = pd.DataFrame(\n",
    "        {\"source\": authors, \"target\": papers}\n",
    "    )\n",
    "    \n",
    "    authors = [\"a-\"+str(p.item()) for p in np.unique(x[0])]\n",
    "    papers = [\"p-\"+str(p.item()) for p in np.unique(x[1])]\n",
    "    a_df = pd.DataFrame(index = authors)\n",
    "    p_df = pd.DataFrame(index =  papers)\n",
    "    break\n",
    "# edges_df[\"label\"] = \"writes\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a_df\n",
    "# len(authors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StellarGraph: Undirected multigraph\n",
      " Nodes: 18385, Edges: 19645\n",
      "\n",
      " Node types:\n",
      "  papers: [14328]\n",
      "    Features: none\n",
      "    Edge types: papers-default->authors\n",
      "  authors: [4057]\n",
      "    Features: none\n",
      "    Edge types: authors-default->papers\n",
      "\n",
      " Edge types:\n",
      "    authors-default->papers: [19645]\n",
      "        Weights: all 1 (default)\n",
      "        Features: none\n"
     ]
    }
   ],
   "source": [
    "a_p_g = sg.StellarGraph({\"authors\": a_df, \"papers\": p_df}, edges_df)\n",
    "print(a_p_g.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StellarGraph: Undirected multigraph\n",
      " Nodes: 78, Edges: 69\n",
      "\n",
      " Node types:\n",
      "  papers: [68]\n",
      "    Features: none\n",
      "    Edge types: papers-default->authors\n",
      "  authors: [10]\n",
      "    Features: none\n",
      "    Edge types: authors-default->papers\n",
      "\n",
      " Edge types:\n",
      "    authors-default->papers: [69]\n",
      "        Weights: all 1 (default)\n",
      "        Features: none\n"
     ]
    }
   ],
   "source": [
    "# type(x[0]), len(x[0])\n",
    "# x = x[x!=value]\n",
    "# edges_df\n",
    "# num_authors.item(), type(num_authors)\n",
    "# list(edges_df.loc[edges_df[\"source\"]==\"a-0\"].target)\n",
    "selected_authors = [\"a-\"+str(x) for x in range(10)]\n",
    "selected_authors_papers = list(edges_df.loc[edges_df[\"source\"].isin(selected_authors)].target)\n",
    "sub_g = a_p_g.subgraph(selected_authors + list(set(selected_authors_papers)))\n",
    "print(sub_g.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "G_all_nx = nx.from_pandas_edgelist(edges_df)#, edge_attr=\"label\")\n",
    "# nx.set_node_attributes(G_all_nx, \"paper\", \"label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StellarGraph: Undirected multigraph\n",
      " Nodes: 18385, Edges: 19645\n",
      "\n",
      " Node types:\n",
      "  default: [18385]\n",
      "    Features: none\n",
      "    Edge types: default-default->default\n",
      "\n",
      " Edge types:\n",
      "    default-default->default: [19645]\n",
      "        Weights: all 1 (default)\n",
      "        Features: none\n"
     ]
    }
   ],
   "source": [
    "G_all = sg.StellarGraph.from_networkx(G_all_nx)\n",
    "print(G_all.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# شاید بهتر باشه به جای زیرگراف، یک ماسک برای اتصالات انتخابی داشته باشیم\n",
    "# چون در ادامه ماسکهای آموزش و اعتبارسنجی و تست رو داریم. \n",
    "# هر نودی که نمی خواهیم در آموزش مشارکت داشته باشه رو میشه در ماسک آموزش لحاظ کرد\n",
    "# def sub_g(edge_index_dict, d_key, indices):\n",
    "# #     d_key: drop key\n",
    "# #     t_key: tuple key\n",
    "#     for t_key, x in edge_index_dict.items():\n",
    "#         if d_key in t_key:\n",
    "            \n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "with np.load('/mnt/c/temp/working/data/DBLP/raw/adjM.npz') as M:\n",
    "    print(M['indices'])\n",
    "    print(M['indptr'])\n",
    "    print(M['format'])\n",
    "    print(M['shape'])\n",
    "    print(M['data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M.files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HeteroData(\n",
      "  \u001b[1mauthor\u001b[0m={\n",
      "    x=[4057, 334],\n",
      "    y=[4057],\n",
      "    train_mask=[4057],\n",
      "    val_mask=[4057],\n",
      "    test_mask=[4057]\n",
      "  },\n",
      "  \u001b[1mpaper\u001b[0m={ x=[14328, 4231] },\n",
      "  \u001b[1mterm\u001b[0m={ x=[7723, 50] },\n",
      "  \u001b[1mconference\u001b[0m={ num_nodes=20 },\n",
      "  \u001b[1m(author, to, paper)\u001b[0m={ edge_index=[2, 19645] },\n",
      "  \u001b[1m(paper, to, author)\u001b[0m={ edge_index=[2, 19645] },\n",
      "  \u001b[1m(paper, to, term)\u001b[0m={ edge_index=[2, 85810] },\n",
      "  \u001b[1m(paper, to, conference)\u001b[0m={ edge_index=[2, 14328] },\n",
      "  \u001b[1m(term, to, paper)\u001b[0m={ edge_index=[2, 85810] },\n",
      "  \u001b[1m(conference, to, paper)\u001b[0m={ edge_index=[2, 14328] }\n",
      ")\n",
      "Epoch: 001, Loss: 1.3775, Train: 0.5575, Val: 0.4275, Test: 0.4965\n",
      "Epoch: 002, Loss: 1.2896, Train: 0.5875, Val: 0.4650, Test: 0.5327\n",
      "Epoch: 003, Loss: 1.1828, Train: 0.7550, Val: 0.5525, Test: 0.6334\n",
      "Epoch: 004, Loss: 1.0495, Train: 0.8175, Val: 0.6275, Test: 0.6798\n",
      "Epoch: 005, Loss: 0.8928, Train: 0.8850, Val: 0.6675, Test: 0.7123\n",
      "Epoch: 006, Loss: 0.7271, Train: 0.9125, Val: 0.6875, Test: 0.7464\n",
      "Epoch: 007, Loss: 0.5652, Train: 0.9525, Val: 0.7125, Test: 0.7660\n",
      "Epoch: 008, Loss: 0.4162, Train: 0.9750, Val: 0.7500, Test: 0.7912\n",
      "Epoch: 009, Loss: 0.2908, Train: 0.9850, Val: 0.7750, Test: 0.8124\n",
      "CPU times: user 5min 23s, sys: 29 s, total: 5min 52s\n",
      "Wall time: 4min 57s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%run test_01.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HeteroData(\n",
      "  \u001b[1mauthor\u001b[0m={\n",
      "    x=[4057, 334],\n",
      "    y=[4057],\n",
      "    train_mask=[4057],\n",
      "    val_mask=[4057],\n",
      "    test_mask=[4057]\n",
      "  },\n",
      "  \u001b[1mpaper\u001b[0m={ x=[14328, 4231] },\n",
      "  \u001b[1mterm\u001b[0m={ x=[7723, 50] },\n",
      "  \u001b[1mconference\u001b[0m={ num_nodes=20 },\n",
      "  \u001b[1m(author, to, paper)\u001b[0m={ edge_index=[2, 19645] },\n",
      "  \u001b[1m(paper, to, author)\u001b[0m={ edge_index=[2, 19645] },\n",
      "  \u001b[1m(paper, to, term)\u001b[0m={ edge_index=[2, 85810] },\n",
      "  \u001b[1m(paper, to, conference)\u001b[0m={ edge_index=[2, 14328] },\n",
      "  \u001b[1m(term, to, paper)\u001b[0m={ edge_index=[2, 85810] },\n",
      "  \u001b[1m(conference, to, paper)\u001b[0m={ edge_index=[2, 14328] }\n",
      ")\n",
      "Epoch: 001, Loss: 1.3807, Train: 0.5081, Val: 0.4425, Test: 0.5100\n",
      "Epoch: 002, Loss: 1.3149, Train: 0.5867, Val: 0.4925, Test: 0.5375\n",
      "Epoch: 003, Loss: 1.2252, Train: 0.7525, Val: 0.6625, Test: 0.7400\n",
      "Epoch: 004, Loss: 1.1170, Train: 0.8106, Val: 0.7450, Test: 0.7900\n",
      "Epoch: 005, Loss: 0.9861, Train: 0.8465, Val: 0.7800, Test: 0.8150\n",
      "Epoch: 006, Loss: 0.8430, Train: 0.8618, Val: 0.8025, Test: 0.8200\n",
      "Epoch: 007, Loss: 0.7004, Train: 0.8784, Val: 0.8225, Test: 0.8250\n",
      "Epoch: 008, Loss: 0.5699, Train: 0.9014, Val: 0.8275, Test: 0.8425\n",
      "Epoch: 009, Loss: 0.4567, Train: 0.9174, Val: 0.8400, Test: 0.8700\n",
      "Epoch: 010, Loss: 0.3642, Train: 0.9269, Val: 0.8325, Test: 0.8825\n",
      "Epoch: 011, Loss: 0.2949, Train: 0.9352, Val: 0.8400, Test: 0.8825\n",
      "Epoch: 012, Loss: 0.2428, Train: 0.9426, Val: 0.8475, Test: 0.8725\n",
      "Epoch: 013, Loss: 0.2037, Train: 0.9478, Val: 0.8500, Test: 0.8725\n",
      "Epoch: 014, Loss: 0.1746, Train: 0.9549, Val: 0.8475, Test: 0.8700\n",
      "Epoch: 015, Loss: 0.1514, Train: 0.9604, Val: 0.8450, Test: 0.8725\n"
     ]
    }
   ],
   "source": [
    "%run test_02.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
